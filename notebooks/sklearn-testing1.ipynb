{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gzip_pickle_data_to_pd(url):\n",
    "\n",
    "# This is required to access raw binary files on Github\n",
    "# i.e. it appends the following to the URL: `?raw=true`.\n",
    "\n",
    "    query = {'raw': 'true'} \n",
    "\n",
    "# Use the requests module to parse the URL with the provided parameters.\n",
    "    response = requests.get(url, params=query)\n",
    "\n",
    "# Create a file pointer initialized to the content of the response\n",
    "# using `BytesIO`. This is a pseudo-file, which can now be read\n",
    "# using `pandas.read_csv`. Since `response.content` is binary data\n",
    "# i.e. bytes, we use `BytesIO`. If the response was text, we would\n",
    "# have used `StringIO`.\n",
    "\n",
    "    fp = BytesIO(response.content)\n",
    "\n",
    "# Finally, parse the content into a DataFrame \n",
    "# (populate other parameters as needed).\n",
    "\n",
    "    df = pd.read_pickle(fp, compression='gzip')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/jess-miles/nyt-engagement/blob/master/data/cleaned_data.pickle.gz\"\n",
    "\n",
    "df1 = read_gzip_pickle_data_to_pd(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/jess-miles/nyt-engagement/blob/master/data/cleaned_article_data.pickle.gz\"\n",
    "\n",
    "df = read_gzip_pickle_data_to_pd(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perhaps need to join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Might need to join these datasets to get all columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sets of columns to be transformed different ways\n",
    "\n",
    "text_col = 'article_text'\n",
    "topic_col = 'topics'\n",
    "cat_cols = ['section_name', 'word_count_cat', 'is_multimedia', 'on_weekend']\n",
    "\n",
    "# initial X and y set for all posts\n",
    "\n",
    "X = df[[text_col, topic_col] + cat_cols]\n",
    "y = df['target']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_trans = ColumnTransformer([\n",
    "    ('txt', TfidfVectorizer(), text_col),\n",
    "    ('txt_kw', CountVectorizer(), topic_col),\n",
    "    ('ohe', OneHotEncoder(drop='first'), cat_cols)], remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('trans', cols_trans),\n",
    "    ('clf', LogisticRegression(max_iter=300, class_weight='balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "# with display='diagram', simply use display() to see the diagram\n",
    "\n",
    "display(pipe)\n",
    "\n",
    "# if desired, set display back to the default\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv_dev_titanic-streamlit-app': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3de96be564863eb70bacda9d696b254a7026b525f8ce08839ccf359adbdf151e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
